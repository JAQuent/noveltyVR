---
title: "Design analysis for noveltyVR"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE)
options(scipen = 30)
```

# Aim of this document
Instead of frequentist statistics as planned previously, we decided to use a Bayesian framework more specifically a Sequential Bayesian Factor (SBF) analysis to examine whether the experience of novelty can enhance memory for unrelated information. 

Nevertheless, I also report the power analysis that I ran, in which I am determining the necessary sample size to find an effect with a power of 95 % based on the results of Fenker et al. (2008). Even though a Bayesian approach will be used, this power analysis provide useful guidance that will be used to justify an upper limit.

# Design
A few words on the design, we want to see whether the experience of novelty can increase memory performance for unrelated information. Fenker at al. (2008) found evidence for that in humans using novel scenes and showed it enhanced memory for information subsequently learned. We plan to examine how the experience of VR something that is in itself seen as something novel affects memory performance for information learned before the expierence itself. We will be using a between group design directly comparing memory performance between VR and a non-VR control group with the Bayesian version of the t-test. 

# Libraries
```{r}
library(ggplot2)
library(pwr)
library(plyr)
library(knitr)
library(grid)
library(gridExtra)
```

# Calculating effect sizes
```{r}
cohensD <- function(m1, sd1, m2, sd2){
  # Calculates the effect size cohen's d based on means and standard deviations
  return((m2 - m1)/sqrt((sd2*sd2 + sd1 * sd1)/2))
}

# From Table 1
effectSize_exp1_remember_immediate <- cohensD(28, 15, 40, 23)
effectSize_exp1_remember_delayed   <- cohensD(16, 10, 30, 19)

# From Table 3
effectSize_exp2_recall_immediate   <- cohensD(45, 9, 55, 12)
```

We are using the effect sizes from Fenker et al. (2008) to plan our study. In total, there were three effects, which are interesting for our analysis. In Experiment 1, the effect size (Cohen's d) for remember judgments was *d* = `r round(effectSize_exp1_remember_immediate, 2)` immediate and *d* = `r round(effectSize_exp1_remember_delayed, 2)` for delayed recognition. I was not able to calculate estimates of recollection and its SD from which I could have calculated an effect size, therefore I used the raw remember rates as proxies. 

The effect size most suitable to base our study on was immediate recall in Experiment 2, which was *d* = `r round(effectSize_exp2_recall_immediate, 2)`. I am going to use this to base my simulation of my design analysis on. It is also the one with the highest effect size. 

There are good reasons to choose free recall as the central measure for the novelty effect of VR. First, it is a straight forward test of memory performance that in contrast o  qualitative memory, it has less theoretical assumptions than remember/recollection and it best suited for our paradigm. 

As a side note, it is also interesting to see that the effect size increased from delayed recognition compared to immediate recognition, which can be interpreted as evidence for consolidation. This is an argument for including delayed recognition to our design. 

# Analysis
## Power analysis (NHST approach)
```{r}
# This is the power I want to have in this experiment
powerAim <- 0.95 

# For Exp1, remember, immediate 
n_exp1_remember_immediate          <- 1
powerVal_exp1_remember_immediate   <- c(0)
while(max(powerVal_exp1_remember_immediate) < powerAim){
   n_exp1_remember_immediate <- n_exp1_remember_immediate + 1
  powerVal_exp1_remember_immediate[n_exp1_remember_immediate] <- pwr.t.test(n = n_exp1_remember_immediate,
                                                        d = effectSize_exp1_remember_immediate,
                                                        type = 'two.sample',
                                                        alternative = 'greater')$power
}
# Replace zero with NA
powerVal_exp1_remember_immediate[1] <- NA

# For Exp1, remember, delayed 
n_exp1_remember_delayed          <- 1
powerVal_exp1_remember_delayed   <- c(0)
while(max(powerVal_exp1_remember_delayed) < powerAim){
   n_exp1_remember_delayed <- n_exp1_remember_delayed + 1
  powerVal_exp1_remember_delayed[n_exp1_remember_delayed] <- pwr.t.test(n = n_exp1_remember_delayed,
                                                        d = effectSize_exp1_remember_delayed,
                                                        type = 'two.sample',
                                                        alternative = 'greater')$power
}
# Replace zero with NA
powerVal_exp1_remember_delayed[1] <- NA

# For Exp2, recall, immediate 
n_exp2_recall_immediate           <- 1
powerVal_exp2_recall_immediate    <- c(0)
while(max(powerVal_exp2_recall_immediate) < powerAim){
   n_exp2_recall_immediate <- n_exp2_recall_immediate + 1
  powerVal_exp2_recall_immediate[n_exp2_recall_immediate] <- pwr.t.test(n = n_exp2_recall_immediate,
                                                        d = effectSize_exp2_recall_immediate,
                                                        type = 'two.sample',
                                                        alternative = 'greater')$power
}
# Replace zero with NA
powerVal_exp2_recall_immediate[1] <- NA

# Concatente to one data frame
powerVal_DF <- data.frame(Effect = c(rep('Exp 1: immediate remember', length(powerVal_exp1_remember_immediate)),
                                   rep('Exp 1: delayed remember', length(powerVal_exp1_remember_delayed)),
                                   rep('Exp 2: immediate recall', length(powerVal_exp2_recall_immediate))),
                          n = c(1:n_exp1_remember_immediate,
                                1:n_exp1_remember_delayed,
                                1:n_exp2_recall_immediate),
                          power = c(powerVal_exp1_remember_immediate,
                                    powerVal_exp1_remember_delayed, 
                                    powerVal_exp2_recall_immediate))

# Plot power curve
ggplot(powerVal_DF, aes(x = n, y = power, colour = Effect)) + geom_line() + 
  geom_hline(yintercept = powerAim) +
  labs(y = 'Estimated power', 
       x = 'Number of participants per group', 
       title = 'Power curves based on Fenker et al. (2008)') + 
  annotate('text', x = 10, y = 0.92, label = '95 % power') +
  coord_cartesian(expand = FALSE, ylim = c(0, 1)) +
  theme(legend.position = c(0.5, 0.2))
``` 

If we used a frequentist approach, we would have 95 % power to detect a significant effect with `r n_exp2_recall_immediate*2` participants in total. In theory, this would also give us `r round(powerVal_exp1_remember_delayed[n_exp2_recall_immediate], 2)*100` % to find an effect for delayed remember judgments as seen in Experiment 1.  However, it is important to note that in contrast to Experiment 1, there was no significant effect of immediate recognition in Experiment 2, so might not find an effect at all because Experiment 1 did not include free recall, which might have affected recognition performance. In any case, 26 participants per group is quite high but still just a feasible number so that I think it's a sensible sample size to use as an upper limit in a SBF design (see below). 

## Bayesian design analysis
For the study, we plan to use a SBF analysis  with an upper limit on the sample size and an endpoint Bayes factor of 10. The upper limit is set to 26 for the reasons mentioned above. The simulation below will help us to determine whether that upper limit will help us to provide conclusive evidence. The design analysis is based on Schönbrodt & Wagenmakers (2018).

I needed a couple of attempts to run the correct simulation for the Bayesian design analysis. In my first attempt (can be found [here](https://github.com/JAQuent/noveltyVR/blob/master/preparation/bayesianDesignAnalysis1.R)), the problem  was that I just sampled Bayes factors for various fixed sample sizes, but I did not actually simulate sequential testing. The data of that simulation can be found [here](https://github.com/JAQuent/noveltyVR/blob/master/preparation/noveltyVR_bayesianDesignAnalysis_20190108_165205.RData). In another [simulation](https://github.com/JAQuent/noveltyVR/blob/master/preparation/noveltyVR_SBF_DesignAnalysis_20190109_165631.RData)
, I ran a simulation with an upper limit of 500 participants per group without realising that I cannot properly calculate the Bayes factor at the upper limit I actually want to select. I then re-ran the [simulation](https://github.com/JAQuent/noveltyVR/blob/master/preparation/noveltyVR_SBF_DesignAnalysis_20190110_103336.RData) then sampling actually realistic sample sizes. However the problem with that was that I did not use the correct hypothesis. The hypotheses in this simulation were $H_0: \mu= 0$ and $H_1: \mu \neq 0$. Since we do not expect the novelty group to show worse memory performance than the control group, I re-ran the analysis with these hypotheses: $H_0: \mu = 0$ and $H_1: \mu > 0$.

The final simulation was run with this [script](https://github.com/JAQuent/noveltyVR/blob/master/preparation/bayesianDesignAnalysis2.R) (see below as well). Its data can be found [here](https://github.com/JAQuent/noveltyVR/blob/master/preparation/noveltyVR_SBF_DesignAnalysis_20190110_162013.RData).

### Simulation script

```{r eval = FALSE}
# This script simulates an Sequential Bayes Factor analysis with an upper limit
# for the project noveltyVR

# Setting seed
set.seed(39257)
 
# Libraries
library(BayesFactor)
library(parallel)

# Functions
cohensD <- function(m1, sd1, m2, sd2){
  # Calculates the effect size cohen's d based on means and standard deviations
  return((m2 - m1)/sqrt((sd2*sd2 + sd1 * sd1)/2))
}

datedFileNam <- function(fileName, fileEnding){
  return(paste(fileName, 
               '_',
               format(Sys.time(), "%Y%m%d_%H%M%S"), 
               fileEnding,
               sep = ''))
}

# One sided hypothesis
# following http://bayesfactor.blogspot.com/2014/02/bayes-factor-t-tests-part-2-two-sample.html
# H0 = 0, H1 > 0
# Open SBF
SBF_tTest_between_one <- function(params){
  nStart      <- params[1]
  nEnd        <- params[2]
  targetBF    <- params[3]
  effectSize  <- params[4]
  
  # Creating populations
  pop1 = rnorm(nEnd, effectSize, 1)
  pop2 = rnorm(nEnd, 0, 1)
  
  # Sequential analysis
  for(n in nStart:length(pop1)){
    bf <- as.numeric(as.vector(ttestBF(pop1[1:n], pop2[1:n], nullInterval = c(0, Inf))[1]))
    if(bf <= 1/targetBF | bf >= targetBF){
      break
    }
  }
  
  return(c(bf, n))
}

# Two sided hypothesis (previously used)
# H0 = 0, H1 != 0
SBF_tTest_between_two <- function(params){
  nStart      <- params[1]
  nEnd        <- params[2]
  targetBF    <- params[3]
  effectSize  <- params[4]
  
  # Creating populations
  pop1 = rnorm(nEnd, effectSize, 1)
  pop2 = rnorm(nEnd, 0, 1)
  
  # Sequential analysis
  for(n in nStart:length(pop1)){
    bf <- as.numeric(as.vector(ttestBF(pop1[1:n], pop2[1:n])))
    if(bf <= 1/targetBF | bf >= targetBF){
      break
    }
  }
  
  return(c(bf, n))
}

# Setting parameters
# From Table 3
effectSize  <- cohensD(45, 9, 55, 12)
nStart      <- 10
targetBF    <- 10
nIterations <- 10000

# SBF version + max
nEnd        <- seq(26, 40, 1)
paramsH0 <- data.frame(nStart     = rep(nStart, nIterations * length(nEnd)),
                       nEnd       = rep(nEnd, nIterations),
                       targetBF   = rep(targetBF, nIterations *length(nEnd)),
                       effectSize = rep(0, nIterations * length(nEnd)))

paramsH1 <- data.frame(nStart     = rep(nStart, nIterations * length(nEnd)),
                       nEnd       = rep(nEnd, nIterations),
                       targetBF   = rep(targetBF, nIterations * length(nEnd)),
                       effectSize = rep(effectSize, nIterations * length(nEnd)))

paramsH0 <- as.matrix(paramsH0)
paramsH1 <- as.matrix(paramsH1)

# Creating cluster
numCores <- detectCores() - 1
cluster  <- makeCluster(numCores)
clusterExport(cluster, 'ttestBF')

# Running analysis
startTime <- Sys.time()
bfH0      <- parRapply(cluster, paramsH0, SBF_tTest_between_one)
time1     <- Sys.time()
bfH1      <- parRapply(cluster, paramsH1, SBF_tTest_between_one)
time2     <- Sys.time()

print(time2)

# Stopping analysis
stopCluster(cluster)

# Saving results
save(startTime, 
     time1,
     time2, 
     paramsH0,
     paramsH1,
     bfH0,
     bfH1,
     file = datedFileNam('noveltyVR_SBF_DesignAnalysis', '.RData'))
```

### Short description of final simulation
In the final run, I simulated a SBF analysis with a minimal sample size of 10 participants per group and an upper limit. The simulation was repeated 10,000 times for all maximal sample sizes between and including 26 and 40 participants. In the simulation, one participant was added in each run until a Bayes factor of 10 or 1/10 or the upper limit was reached. Checking the statistics after each added participant does increase the probability of misleading evidence (Schönbrodt & Wagenmakers, 2018). Basing our sample size considerations on this simulation is therefore more conservative because in reality sample sizes will only be checked at the end of day rather than after each testing session, which will include more than one participant. The simulation was based on the effect size of immediate recall of Experiment 2 (*d* = `r round(effectSize_exp2_recall_immediate, 2)`). 

### Analysing results
Following Schönbrodt & Wagenmakers (2018), we want to answer the following questions with that design analysis:

1. How many studies stop because evidence or reaching the upper limit?

2. What is the probability of misleading evidence?

3. If stopped at the upper limit, how many studies point in the right direction?

4. How will the distribution of sample sizes look like?

```{r}
# Loading the data
load('U:/Projects/noveltyVR/preparation/noveltyVR_SBF_DesignAnalysis_20190110_162013.RData')
```

The simulation itself was run on computer cluster with 16 CPUs, which took `r round(time2 - startTime, 2)` hours to complete using R version 3.2.2.

```{r}
# Preparing the data for analysis
dataH0 <- data.frame(BF = bfH0[seq(1, dim(paramsH0)[1]*2, 2)],
                     n  = bfH0[seq(2, dim(paramsH0)[1]*2, 2)])
dataH1 <- data.frame(BF = bfH1[seq(1, dim(paramsH0)[1]*2, 2)],
                     n  = bfH1[seq(2, dim(paramsH0)[1]*2, 2)])

# Set upper limit
upperLimit <- 26

# Subset to upperlimit
dataH0 <- dataH0[paramsH0[, 2] == upperLimit,]
dataH1 <- dataH1[paramsH1[, 2] == upperLimit,]

# Other values
targetBF    <- 10
nIterations <- table(paramsH0[, 2])[1]
```

### 1. How many studies stop because evidence or reaching the upper limit?
```{r}
# For H0
studies_evidenceStopped_H0 <- sum(dataH0$n < upperLimit & (dataH0$BF >= targetBF | dataH0$BF <= 1/targetBF))
studies_limitStopped_H0    <- nIterations - studies_evidenceStopped_H0
# Calculating percentages
studies_evidenceStopped_H0 <- studies_evidenceStopped_H0/nIterations
studies_limitStopped_H0    <- studies_limitStopped_H0/nIterations

# For H1
studies_evidenceStopped_H1 <- sum(dataH1$n < upperLimit & (dataH1$BF >= targetBF | dataH1$BF <= 1/targetBF))
studies_limitStopped_H1    <- nIterations - studies_evidenceStopped_H1
# Calculating percentages
studies_evidenceStopped_H1 <- studies_evidenceStopped_H1/nIterations
studies_limitStopped_H1    <- studies_limitStopped_H1/nIterations
```

Based on an upper limit of `r upperLimit` participants per group, `r round(studies_evidenceStopped_H0, 2)*100` % stopped based on evidence and `r round(studies_limitStopped_H0, 2)*100` % based on the fact that the maximal sample size was reached under $H_0$. While `r round(studies_evidenceStopped_H1, 2)*100` % stopped based on evidence and `r round(studies_limitStopped_H1, 2)*100` % based on the fact that the maximal sample size was reached under $H_1$. So with `r upperLimit*2` participants in total we are very likely to provide conclusive evidence if the $H_1$ is true. If $H_0$ is true, then evidence will be less convincing but this is an inherent problem of providing evidence for $H_0$. 

### 2. What is the probability of misleading evidence?
```{r}
# For H0
fpe <- sum(dataH0$BF >= targetBF)/nIterations

# For H1
fne <- sum(dataH1$BF <= 1/targetBF)/nIterations
```

The false positive rate under $H_0$ is `r round(fpe, 2)*100` %, while the false negative rate under $H_1$ is `r round(fne, 2)*100` %. Under both hypotheses the probability of misleading evidence is very low. 

### 3. If stopped at the upper limit, how many studies point in the right direction?
```{r}
# For H0
rightDirection_H0 <- sum(dataH0$n == upperLimit & dataH0$BF < 1) / (sum(dataH0$n == upperLimit & dataH0$BF < 1) + sum(dataH0$n == upperLimit & dataH0$BF >= 1))

# For H1
rightDirection_H1 <- sum(dataH1$n == upperLimit & dataH1$BF > 1) / (sum(dataH1$n == 26 & dataH1$BF <= 1) + sum(dataH1$n == upperLimit & dataH1$BF > 1))
```

If data collection is stopped at the upper limit, then `r round(rightDirection_H0, 2)*100` % of the Bayes factors show in the right direction under $H_0$ and `r round(rightDirection_H1, 2)*100` % under $H_1$. This shows that even if the rigorous evidence level is not reached after collection `r upperLimit` per group, we will be still very likely to get a Bayes factor in the correct direction. 

### 4. How will the distribution of sample sizes look like?
```{r}
# For H0
# In order to make the areas of Bayes factor > 1 and < 1 equal I transform the values
# and relabel the y-axis
dataH0$n    <- as.factor(dataH0$n)   
dataH0_trans <- dataH0
dataH0_trans$BF[dataH0_trans$BF < 1] <- -1/dataH0_trans$BF[dataH0_trans$BF < 1] 

plot1 <- ggplot(dataH0, aes(x = n)) + geom_bar() +
  labs(y = 'Count',
       title = bquote('Distribution of sample sizes under '*H[0])) +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank())

plot2 <- ggplot(dataH0_trans, aes(x =  n, y = BF)) + 
  geom_boxplot() +
  geom_hline(yintercept = targetBF) + 
  geom_hline(yintercept = 0) + 
  geom_hline(yintercept = -targetBF) +
  labs(y = 'Bayes factor', 
       x = 'Number of participants per group',
       title = bquote('Distribution of Bayes factors by sample size under '*H[0])) +
  scale_y_continuous(breaks = c(-10, -3, 0, 3, 10), 
                     labels = c('1/10', '1/3', '1', '3', '10'),
                     limits = c(-14, 14))

grid.arrange(plot1,
             plot2,
             ncol = 1)
```

The plot above shows the simulation for the upper limit of 26 participants under $H_0$. The top half of plot shows the number of runs that end with the respective sample size shown on the x-axis. The vast majority of runs did not reach the evidence criterion. With sample sizes over 21, the Bayes factor consistently show in right direction as indicated by box plots in the second halt of the plot. If $H_0$ is true most runs stop at the upper limit but still result in a median Bayes factor of `r round( median(dataH0$BF[dataH0$n == upperLimit]), 2)`, which more than anecdotal evidence. 

```{r}
# For H1
# In order to make the areas of Bayes factor > 1 and < 1 equal I transform the values
dataH1$n     <- as.factor(dataH1$n)   
dataH1_trans <- dataH1
dataH1_trans$BF[dataH1_trans$BF < 1] <- -1/dataH1_trans$BF[dataH1_trans$BF < 1] 

plot3<- ggplot(dataH1, aes(x = n)) + geom_bar() +
  labs(y = 'Count',
       title = bquote('Distribution of sample sizes under '*H[1])) +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank())

plot4 <- ggplot(dataH1_trans, aes(x =  n, y = BF)) + 
  geom_boxplot() +
  geom_hline(yintercept = targetBF) + 
  geom_hline(yintercept = 0) + 
  geom_hline(yintercept = -targetBF) +
  labs(y = 'Bayes factor', 
       x = 'Number of participants per group',
       title = bquote('Distribution of Bayes factors by sample size under '*H[1])) +
  scale_y_continuous(breaks = c(-10, -3, 0, 3, 10), 
                     labels = c('1/10', '1/3', '1', '3', '10'),
                     limits = c(-14, 14))

grid.arrange(plot3,
             plot4,
             ncol = 1)
```

The majority of runs did not even reach the the upper limit as shown in the top half of the plot. Even in small samples, Bayes factors reached the correct evidence criterion. If a run stopped at the upper limit,  the median Bayes factor of `r round( median(dataH0$BF[dataH0$n == upperLimit]), 2)` again provides more than anecdotal evidence. If $H_1$ is true, `r upperLimit` participants per group are enough to provide conclusive evidence for it. 

# Conclusion
All in all, the practical limit of `r upperLimit` participants per group seems to be enough to provide conclusive evidence (esp. for $H_1$) in our planned study. This is evident in the expected number of studies stopping at the evidence criterion, the low probability of misleading evidence (for both $H_0$ and $H_1$) and the general directions of the Bayes factors.

# References
Fenker, D. B., Frey, J. U., Schuetze, H., Heipertz, D., Heinze, H.-J., & Düzel, E. (2008). Novel scenes improve recollection and recall of words. Journal of Cognitive Neuroscience, 20(7), 1250–1265. https://doi.org/10.1162/jocn.2008.20086

Schönbrodt, F. D., & Wagenmakers, E.-J. (2018). Bayes factor design analysis: Planning for compelling evidence. Psychonomic Bulletin & Review, 25(1), 128–142. https://doi.org/10.3758/s13423-017-1230-y